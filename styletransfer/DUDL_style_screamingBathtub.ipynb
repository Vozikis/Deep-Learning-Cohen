{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Style transfer\n",
    "### LECTURE: Transfering the screaming bathtub\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:05.779188Z",
     "start_time": "2023-02-07T20:38:05.623200Z"
    },
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/gq8xgm9x491cqwtct_tgpy0h0000gn/T/ipykernel_87904/605019926.py:19: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('cuda')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "supported formats are: 'jpeg','png2x','png','jpg','retina','svg','pdf' not 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_matplotlib_formats\n\u001b[0;32m---> 19\u001b[0m \u001b[43mset_matplotlib_formats\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dfl_test/lib/python3.8/site-packages/IPython/core/display.py:1237\u001b[0m, in \u001b[0;36mset_matplotlib_formats\u001b[0;34m(*formats, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`set_matplotlib_formats` is deprecated since IPython 7.23, directly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse `matplotlib_inline.backend_inline.set_matplotlib_formats()`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1230\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1231\u001b[0m )\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib_inline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_inline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1234\u001b[0m     set_matplotlib_formats \u001b[38;5;28;01mas\u001b[39;00m set_matplotlib_formats_orig,\n\u001b[1;32m   1235\u001b[0m )\n\u001b[0;32m-> 1237\u001b[0m \u001b[43mset_matplotlib_formats_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dfl_test/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py:286\u001b[0m, in \u001b[0;36mset_matplotlib_formats\u001b[0;34m(*formats, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m kw\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    285\u001b[0m shell \u001b[38;5;241m=\u001b[39m InteractiveShell\u001b[38;5;241m.\u001b[39minstance()\n\u001b[0;32m--> 286\u001b[0m \u001b[43mselect_figure_formats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dfl_test/lib/python3.8/site-packages/IPython/core/pylabtools.py:276\u001b[0m, in \u001b[0;36mselect_figure_formats\u001b[0;34m(shell, formats, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m bad])\n\u001b[1;32m    275\u001b[0m     gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m supported])\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported formats are: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (gs, bs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m formats:\n\u001b[1;32m    279\u001b[0m     png_formatter\u001b[38;5;241m.\u001b[39mfor_type(\n\u001b[1;32m    280\u001b[0m         Figure, partial(print_figure, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m, base64\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    281\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: supported formats are: 'jpeg','png2x','png','jpg','retina','svg','pdf' not 'cuda'"
     ]
    }
   ],
   "source": [
    "### import libraries\n",
    "\n",
    "# for DL modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# to read an image from a url\n",
    "from imageio import imread\n",
    "\n",
    "# for number-crunching\n",
    "import numpy as np\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN5Y7ABfeoNt"
   },
   "source": [
    "# Import VGG19 and freeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:06.773006Z",
     "start_time": "2023-02-07T20:38:06.009335Z"
    },
    "id": "f2FoXgvhesHF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the model\n",
    "vggnet = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# freeze all layers\n",
    "for p in vggnet.parameters():\n",
    "    p.requires_grad = False\n",
    "  \n",
    "# set to evaluation mode\n",
    "vggnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:06.777105Z",
     "start_time": "2023-02-07T20:38:06.774334Z"
    },
    "id": "EeTXUCtOesdA"
   },
   "outputs": [],
   "source": [
    "# send the network to the GPU if available\n",
    "device = torch.device('mps') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "vggnet.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08UaJc5WfV1k"
   },
   "source": [
    "# Import two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.037010Z",
     "start_time": "2023-02-07T20:38:07.956416Z"
    },
    "id": "ZI2WXfdkfYYX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/gq8xgm9x491cqwtct_tgpy0h0000gn/T/ipykernel_87904/270550509.py:1: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img4content = imread('https://upload.wikimedia.org/wikipedia/commons/6/61/De_nieuwe_vleugel_van_het_Stedelijk_Museum_Amsterdam.jpg')\n",
      "/var/folders/f2/gq8xgm9x491cqwtct_tgpy0h0000gn/T/ipykernel_87904/270550509.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img4style   = imread('https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1675, 3000, 3)\n",
      "(1675, 3000, 3)\n",
      "(4000, 3223, 3)\n"
     ]
    }
   ],
   "source": [
    "img4content = imread('https://upload.wikimedia.org/wikipedia/commons/6/61/De_nieuwe_vleugel_van_het_Stedelijk_Museum_Amsterdam.jpg')\n",
    "img4style   = imread('https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg')\n",
    "\n",
    "# initialize the target image and random numbers\n",
    "img4target = np.random.randint(low=0,high=255,size=img4content.shape,dtype=np.uint8)\n",
    "\n",
    "print(img4content.shape)\n",
    "print(img4target.shape)\n",
    "print(img4style.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.130158Z",
     "start_time": "2023-02-07T20:38:12.038107Z"
    },
    "id": "UnKZOcn8esgP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 458])\n",
      "torch.Size([1, 3, 256, 458])\n",
      "torch.Size([1, 3, 317, 256])\n"
     ]
    }
   ],
   "source": [
    "## These images are really large, which will make training take a long time. \n",
    "\n",
    "# create the transforms\n",
    "Ts = T.Compose([ T.ToPILImage(),\n",
    "                 T.Resize(256),\n",
    "                 T.ToTensor(),\n",
    "                 T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "               ])\n",
    "\n",
    "# apply them to the images (\"unsqueeze\" to make them a 4D tensor) and push to GPU\n",
    "img4content = Ts( img4content ).unsqueeze(0).to(device)\n",
    "img4style   = Ts( img4style   ).unsqueeze(0).to(device)\n",
    "img4target  = Ts( img4target  ).unsqueeze(0).to(device)\n",
    "\n",
    "print(img4content.shape)\n",
    "print(img4target.shape)\n",
    "print(img4style.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.156373Z",
     "start_time": "2023-02-07T20:38:12.131096Z"
    },
    "id": "nCL4gmObesjX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's have a look at the \"before\" pics\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,6))\n",
    "\n",
    "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[0].imshow(pic)\n",
    "ax[0].set_title('Content picture')\n",
    "\n",
    "pic = img4target.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[1].imshow(pic)\n",
    "ax[1].set_title('Target picture')\n",
    "\n",
    "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[2].imshow(pic)\n",
    "ax[2].set_title('Style picture')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQnjY4ojjIbH"
   },
   "source": [
    "# Functions to extract image feature map activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.160986Z",
     "start_time": "2023-02-07T20:38:12.158319Z"
    },
    "id": "gX2GQHEIjbSH"
   },
   "outputs": [],
   "source": [
    "# A function that returns feature maps\n",
    "\n",
    "def getFeatureMapActs(img,net):\n",
    "  \n",
    "  # initialize feature maps as a list\n",
    "  featuremaps = []\n",
    "  featurenames = []\n",
    "\n",
    "  convLayerIdx = 0\n",
    "\n",
    "  # loop through all layers in the \"features\" block\n",
    "  for layernum in range(len(net.features)):\n",
    "    \n",
    "    # print out info from this layer\n",
    "    # print(layernum,net.features[layernum])\n",
    "\n",
    "    # process the image through this layer\n",
    "    img = net.features[layernum](img)\n",
    "\n",
    "    # store the image if it's a conv2d layer\n",
    "    if 'Conv2d' in str(net.features[layernum]):\n",
    "      featuremaps.append( img )\n",
    "      featurenames.append( 'ConvLayer_' + str(convLayerIdx) )\n",
    "      convLayerIdx += 1\n",
    "  \n",
    "  return featuremaps,featurenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.164158Z",
     "start_time": "2023-02-07T20:38:12.161948Z"
    },
    "id": "UlcDsv-r5P0H"
   },
   "outputs": [],
   "source": [
    "# A function that returns the Gram matrix of the feature activation map\n",
    "\n",
    "def gram_matrix(M):\n",
    "  \n",
    "  # reshape to 2D\n",
    "  _,chans,height,width = M.shape\n",
    "  M = M.reshape(chans,height*width)  \n",
    "\n",
    "  # compute and return covariance matrix\n",
    "  gram = torch.mm(M,M.t()) / (chans*height*width)\n",
    "  return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.340209Z",
     "start_time": "2023-02-07T20:38:12.165328Z"
    },
    "id": "6RvCMiW13mmn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map \"ConvLayer_0\" is size torch.Size([1, 64, 256, 458])\n",
      "Feature map \"ConvLayer_1\" is size torch.Size([1, 64, 256, 458])\n",
      "Feature map \"ConvLayer_2\" is size torch.Size([1, 128, 128, 229])\n",
      "Feature map \"ConvLayer_3\" is size torch.Size([1, 128, 128, 229])\n",
      "Feature map \"ConvLayer_4\" is size torch.Size([1, 256, 64, 114])\n",
      "Feature map \"ConvLayer_5\" is size torch.Size([1, 256, 64, 114])\n",
      "Feature map \"ConvLayer_6\" is size torch.Size([1, 256, 64, 114])\n",
      "Feature map \"ConvLayer_7\" is size torch.Size([1, 256, 64, 114])\n",
      "Feature map \"ConvLayer_8\" is size torch.Size([1, 512, 32, 57])\n",
      "Feature map \"ConvLayer_9\" is size torch.Size([1, 512, 32, 57])\n",
      "Feature map \"ConvLayer_10\" is size torch.Size([1, 512, 32, 57])\n",
      "Feature map \"ConvLayer_11\" is size torch.Size([1, 512, 32, 57])\n",
      "Feature map \"ConvLayer_12\" is size torch.Size([1, 512, 16, 28])\n",
      "Feature map \"ConvLayer_13\" is size torch.Size([1, 512, 16, 28])\n",
      "Feature map \"ConvLayer_14\" is size torch.Size([1, 512, 16, 28])\n",
      "Feature map \"ConvLayer_15\" is size torch.Size([1, 512, 16, 28])\n"
     ]
    }
   ],
   "source": [
    "# inspect the output of the function\n",
    "featmaps,featnames = getFeatureMapActs(img4content,vggnet)\n",
    "\n",
    "# print out some info\n",
    "for i in range(len(featnames)):\n",
    "  print('Feature map \"%s\" is size %s'%(featnames[i],(featmaps[i].shape)))\n",
    "#second parameter is the num of feature mapxs\n",
    "#third and forth is the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.640976Z",
     "start_time": "2023-02-07T20:38:12.341272Z"
    },
    "id": "AbKb21yP2ujR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see what the \"content\" image looks like\n",
    "contentFeatureMaps,contentFeatureNames = getFeatureMapActs(img4content,vggnet)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
    "for i in range(5):\n",
    "\n",
    "  # average over all feature maps from this layer, and normalize\n",
    "  pic = np.mean( contentFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[0,i].imshow(pic,cmap='gray')\n",
    "  axs[0,i].set_title('Content layer ' + str(contentFeatureNames[i]))\n",
    "\n",
    "\n",
    "  ### now show the gram matrix\n",
    "  pic = gram_matrix(contentFeatureMaps[i]).cpu().numpy()\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[1,i].imshow(pic,cmap='gray',vmax=.1)\n",
    "  axs[1,i].set_title('Gram matrix, layer ' + str(contentFeatureNames[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.987684Z",
     "start_time": "2023-02-07T20:38:12.641946Z"
    },
    "id": "VgLRHR_Cesmf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# repeat for the \"style\" image\n",
    "styleFeatureMaps,styleFeatureNames = getFeatureMapActs(img4style,vggnet)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
    "for i in range(5):\n",
    "\n",
    "  # average over all feature maps from this layer, and normalize\n",
    "  pic = np.mean( styleFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[0,i].imshow(pic,cmap='hot')\n",
    "  axs[0,i].set_title('Style layer ' + str(styleFeatureNames[i]))\n",
    "\n",
    "\n",
    "  ### now show the gram matrix\n",
    "  pic = gram_matrix(styleFeatureMaps[i]).cpu().numpy()\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[1,i].imshow(pic,cmap='hot',vmax=.1)\n",
    "  axs[1,i].set_title('Gram matrix, layer ' + str(styleFeatureNames[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfXfw0Zb5QGe"
   },
   "source": [
    "# Now for the transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:38:12.990871Z",
     "start_time": "2023-02-07T20:38:12.988769Z"
    },
    "id": "FHZAzPCOHWQx"
   },
   "outputs": [],
   "source": [
    "# which layers to use\n",
    "layers4content = [ 'ConvLayer_1','ConvLayer_4' ]\n",
    "layers4style   = [ 'ConvLayer_1','ConvLayer_2','ConvLayer_3','ConvLayer_4','ConvLayer_5' ]\n",
    "weights4style  = [      1       ,     .5      ,     .5      ,     .2      ,     .1       ]\n",
    "#the deeper we get into the network the style contributes less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:45:32.596069Z",
     "start_time": "2023-02-07T20:38:12.992920Z"
    },
    "id": "8_gMykBE5QJI"
   },
   "outputs": [],
   "source": [
    "# make a copy of the target image and push to GPU\n",
    "target = img4target.clone()\n",
    "target.requires_grad = True\n",
    "target = target.to(device)\n",
    "styleScaling = 1e6 #scale up the loss function for the style to make it more comparable\n",
    " \n",
    "# number of epochs to train\n",
    "numepochs = 1500\n",
    "\n",
    "# optimizer for backprop\n",
    "optimizer = torch.optim.RMSprop([target],lr=.005)\n",
    "\n",
    "\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # extract the target feature maps\n",
    "  targetFeatureMaps,targetFeatureNames = getFeatureMapActs(target,vggnet)\n",
    "\n",
    "\n",
    "  # initialize the individual loss components\n",
    "  styleLoss = 0\n",
    "  contentLoss = 0\n",
    "\n",
    "  # loop over layers\n",
    "  for layeri in range(len(targetFeatureNames)):\n",
    "\n",
    "\n",
    "    # compute the content loss\n",
    "    if targetFeatureNames[layeri] in layers4content:\n",
    "      contentLoss += torch.mean( (targetFeatureMaps[layeri]-contentFeatureMaps[layeri])**2 )\n",
    "\n",
    "\n",
    "    # compute the style loss\n",
    "    if targetFeatureNames[layeri] in layers4style:\n",
    "      \n",
    "      # Gram matrices\n",
    "      Gtarget = gram_matrix(targetFeatureMaps[layeri])\n",
    "      Gstyle  = gram_matrix(styleFeatureMaps[layeri])\n",
    "\n",
    "      # compute their loss (de-weighted with increasing depth)\n",
    "      styleLoss += torch.mean( (Gtarget-Gstyle)**2 ) * weights4style[layers4style.index(targetFeatureNames[layeri])]\n",
    "\n",
    "  \n",
    "  # combined loss\n",
    "  combiloss = styleScaling*styleLoss + contentLoss\n",
    "\n",
    "  # finally ready for backprop!\n",
    "  optimizer.zero_grad()\n",
    "  combiloss.backward()\n",
    "  optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_o6RsCi5QLz"
   },
   "source": [
    "# Let's have a looksie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:45:32.628458Z",
     "start_time": "2023-02-07T20:45:32.598534Z"
    },
    "id": "acf88v0EDYic"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1100 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the \"after\" pic\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,11))\n",
    "\n",
    "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[0].imshow(pic)\n",
    "ax[0].set_title('Content picture',fontweight='bold')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "pic = torch.sigmoid(target).cpu().detach().squeeze().numpy().transpose((1,2,0))\n",
    "ax[1].imshow(pic)\n",
    "ax[1].set_title('Target picture',fontweight='bold')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[2].imshow(pic,aspect=.6)\n",
    "ax[2].set_title('Style picture',fontweight='bold')\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMqnRgur6k9A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ_c6cWyIu_x"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfvdVYtsIvB6"
   },
   "outputs": [],
   "source": [
    "# 1) The minimization loss has two components (style and content). Modify the code to store these two components in a\n",
    "#    Nx2 matrix (for N training epochs). Then plot them. This will help you understand and adjust the styleScaling gain\n",
    "#    factor.\n",
    "# \n",
    "# 2) Change the layers for minimizing losses to content and style images. Do you notice an effect of minimizing the\n",
    "#    earlier vs. later layers? How about more vs. fewer layers?\n",
    "# \n",
    "# 3) It's pretty neat to see the target image evolve over time. Modify the code to save the target image every, e.g.,\n",
    "#    100 epochs. Then you can make a series of images showing how the noise transforms into a lovely picture.\n",
    "# \n",
    "# 4) The target picture was initialized as random noise. But it doesn't need to be. It can be initialized to anything\n",
    "#    else. Try the following target initializations: (1) the content picture; (2) the style picture; (3) a completely\n",
    "#    different picture (e.g., a picture of you or a cat or the Taj Mahal).\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNE9y2IWWpNrq1+kg0hqSmA",
   "collapsed_sections": [],
   "name": "DUDL_style_screamingBathtub.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
